{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JShah\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/processed/train.csv', delimiter=',')\n",
    "df_test = pd.read_csv('../data/processed/test.csv', delimiter=',')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max qid =  537932\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_max_qid(df):\n",
    "    max_qid = 0\n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid1>max_qid:\n",
    "            max_qid = qid1\n",
    "        elif qid2>max_qid:\n",
    "            max_qid = qid2\n",
    "    print('Max qid = ', max_qid)\n",
    "    return max_qid\n",
    "\n",
    "max_qid = get_max_qid(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_questions(df):\n",
    "    group_id = 0\n",
    "    group_list = np.repeat(-1, max_qid)\n",
    "    \n",
    "    for idx,frame in df.iterrows():\n",
    "        qid1 = int(frame['qid1'])\n",
    "        qid2 = int(frame['qid2'])\n",
    "        \n",
    "        if int(frame['is_duplicate'])==1:\n",
    "            # if both has no group, add new group\n",
    "            if group_list[qid1]==-1 and group_list[qid2]==-1:\n",
    "                group_list[qid1] = group_id\n",
    "                group_list[qid2] = group_id\n",
    "                group_id += 1\n",
    "\n",
    "            # if both has group, join the group \n",
    "            elif group_list[qid1]!=-1 and group_list[qid2]!=-1 :\n",
    "                idxes_to_be_joined = np.where(group_list==group_list[qid2])[0]\n",
    "                group_list[idxes_to_be_joined] = group_list[qid1]\n",
    "\n",
    "            # only q1 has group , than add q2 to q1's group\n",
    "            elif  group_list[qid1]!=-1:\n",
    "                group_list[qid2] = group_list[qid1]\n",
    "\n",
    "            # only q2 has group , than add q1 to q2's group\n",
    "            elif  group_list[qid2]!=-1:\n",
    "                group_list[qid1] = group_list[qid2]\n",
    "                \n",
    "    return group_list\n",
    "    \n",
    "group_ids = group_questions(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149650"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(group_ids!=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     1 ... 64024 64025 64025]\n"
     ]
    }
   ],
   "source": [
    "print(group_ids[group_ids != -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the group and store it as a dictionary\n",
    "group_dict = {}\n",
    "for i in range(np.max(group_ids)+1):\n",
    "    group_members = np.where(group_ids==i)[0]\n",
    "    if len(group_members)>0:\n",
    "        group_dict[i] = group_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def enumerate_all_positive_cases(group_dict):\n",
    "    \n",
    "    def enumerate_inside_group(group):\n",
    "        return list(itertools.combinations(group, 2))\n",
    "    \n",
    "    return np.vstack(enumerate_inside_group(group_dict[group_id]) for group_id in group_dict)\n",
    "\n",
    "def duplicate_all(df):\n",
    "    \n",
    "    def get_qid_set():\n",
    "        ids = set()\n",
    "        for i,series in df.iterrows():\n",
    "            if series['qid1'] not in ids:\n",
    "                ids.add(series['qid1'])\n",
    "            if series['qid2'] not in ids:\n",
    "                ids.add(series['qid2'])\n",
    "        return ids\n",
    "    \n",
    "    id_set = get_qid_set()\n",
    "    return [[i,i] for i in id_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JShah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Enumerate all cases of duplicated question pairs from each group\n",
    "enumerate_pairs = enumerate_all_positive_cases(group_dict)\n",
    "\n",
    "# The question pairs with itself is also a sample of duplicated question pair\n",
    "# duplicate_pairs = duplicate_all(df_train) \n",
    "\n",
    "all_pos_pairs = enumerate_pairs\n",
    "# all_pos_pairs = np.vstack([enumerate_pairs,duplicate_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duplicate question pair count grows from 149263 to 228548\n"
     ]
    }
   ],
   "source": [
    "print('The duplicate question pair count grows from {} to {}'.format(len(df_train[df_train['is_duplicate']==1]),len(all_pos_pairs))) # The total duplicated samples count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_rate_in_training_set():\n",
    "    dup = np.array(df_train['is_duplicate'])\n",
    "    pos_ratio = np.sum(dup) / dup.shape[0]\n",
    "    return pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are very probable to use a single question for several times, we should remove validation samples directly at this point\n",
    "# Fro example, Q_a == Q_b and Q_b == Q_c ,\n",
    "# In my method, we'll generate a new data Q_a == Q_c\n",
    "# If we move this Q_a == Q_c sample to validation set,\n",
    "#     it is very weird that our training set already has this kind of information (can be recognized from the Q_a == Q_b == Q_c relation).\n",
    "\n",
    "import random\n",
    "\n",
    "validation_size = 20000 # an approximation, final result can be slightly more than this number\n",
    "\n",
    "def split_val(pos_pairs, val_pos_ratio):\n",
    "    \n",
    "    # an estimation of how many data should be split from training set\n",
    "    split_pos_size = int(validation_size * val_pos_ratio)\n",
    "    \n",
    "    qids = pos_pairs.flatten()\n",
    "    \n",
    "    # totally remove those selected qids from training set\n",
    "    val_bools = np.repeat(False,pos_pairs.shape[0])\n",
    "    while(np.sum(val_bools)<split_pos_size):\n",
    "        rnd_qid = random.randint(0,len(qids)-1)\n",
    "        val_single_bool = np.bitwise_or(pos_pairs[:,0]==rnd_qid,pos_pairs[:,1]==rnd_qid)\n",
    "        val_bools = np.bitwise_or(val_bools, val_single_bool)\n",
    "    val_idxes = np.where(val_bools)[0]\n",
    "    \n",
    "    val = pos_pairs[val_idxes]\n",
    "    train = np.delete(pos_pairs, val_idxes, axis=0)\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_ratio = get_pos_rate_in_training_set()\n",
    "train_pos_pairs, val_pos_pairs = split_val(all_pos_pairs, val_pos_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training positive pairs: 221158\n",
      "Validation positive pairs: 7390\n"
     ]
    }
   ],
   "source": [
    "print('Training positive pairs:', train_pos_pairs.shape[0])\n",
    "print('Validation positive pairs:', val_pos_pairs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_pos_pairs, open('../data/processed/train_positive_qid_pairs.pkl', 'wb'))\n",
    "pickle.dump(val_pos_pairs, open('../data/processed/validation_positive_qid_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicate = df_train[df_train['is_duplicate']==0]\n",
    "non_dup_question_pairs = np.array([[series['qid1'],series['qid2']] for i,series in non_duplicate.iterrows()])\n",
    "\n",
    "val_pos_count = val_pos_pairs.shape[0]\n",
    "val_total_count = val_pos_count / val_pos_ratio\n",
    "val_neg_count = int(val_total_count - val_pos_count)\n",
    "\n",
    "val_neg_idxes = [random.randint(0,len(non_dup_question_pairs)-1) for i in range(val_neg_count)]\n",
    "\n",
    "val_neg_pairs = non_dup_question_pairs[val_neg_idxes]\n",
    "train_neg_pairs = np.delete(non_dup_question_pairs, val_neg_idxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauranteed non-duplicated question pair length is  255027 \n",
      "\n",
      "Training negative pairs: 242705\n",
      "Validation negative pairs: 12626\n"
     ]
    }
   ],
   "source": [
    "print('Gauranteed non-duplicated question pair length is ', len(non_dup_question_pairs), '\\n')\n",
    "\n",
    "print('Training negative pairs:', train_neg_pairs.shape[0])\n",
    "print('Validation negative pairs:', val_neg_pairs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(train_pos_pairs, open('../data/processed/train_negative_qid_pairs.pkl', 'wb'))\n",
    "pickle.dump(val_pos_pairs, open('../data/processed/validation_negative_qid_pairs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation final size: 20016\n"
     ]
    }
   ],
   "source": [
    "print('Validation final size:', val_neg_pairs.shape[0] + val_pos_pairs.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv('../data/processed/train.csv', delimiter=',')\n",
    "# enc_map = pickle.load(open('../data/processed/enc_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_wrod_list(question):\n",
    "    \n",
    "    if type(question)!=str:\n",
    "        return []\n",
    "    \n",
    "    # identify special characters that separate words : (space) ' ! \" ? @ ^ + * / . , ~ ( ) [ ] { } & | ` $ % = : ; < >  \n",
    "    special_chars = '[\\s\\'!\"\\?@\\^+*/\\.,~\\(\\)\\[\\]\\{\\}\\&\\|`\\$\\%\\=:;\\<\\>\\-]'\n",
    "    pre_separator = '(?='+special_chars+')'\n",
    "    post_separator = '(?='+special_chars+'|$)'\n",
    "    single_word = '[^\\s\\-]+' # non-empty is enough here\n",
    "\n",
    "    return re.findall(special_chars+single_word+post_separator, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_qid_question_dict(df, parse=False):\n",
    "    res = {}\n",
    "    for i,frame in df.iterrows():\n",
    "        \n",
    "        qid1 = int(frame['qid1'])\n",
    "        if qid1 not in res:\n",
    "            if parse:\n",
    "                res[qid1] = parse_wrod_list(frame['question1'])\n",
    "            else:\n",
    "                res[qid1] = frame['question1']\n",
    "        \n",
    "        qid2 = int(frame['qid2'])\n",
    "        if qid2 not in res:\n",
    "            if parse:\n",
    "                res[qid2] = parse_wrod_list(frame['question2'])\n",
    "            else:\n",
    "                res[qid2] = frame['question2']\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_question_dict = gen_qid_question_dict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(training_question_dict, open('../data/processed/qid_question_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
