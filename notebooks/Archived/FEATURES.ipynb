{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpaCy `en_core_web_md` corpus...\n"
     ]
    }
   ],
   "source": [
    "# Import spacy corpus, glove embeddings.\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "\n",
    "\n",
    "# !spacy download en_core_web_sm\n",
    "# !pip install textacy\n",
    "# import textacy\n",
    "# import 'en_core_web_md'\n",
    "print('Loading SpaCy `en_core_web_md` corpus...')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JShah\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# q1_train_simple, q2_train_simple, y_train = pickle.load(open('./lystdo_kernel/train_text_without_process.pkl','rb'))\n",
    "# q1_test_simple, q2_test_simple, y_test = pickle.load(open('./lystdo_kernel/test_text_without_process.pkl','rb'))\n",
    "\n",
    "df_train = pd.read_csv('../data/processed/train.csv')\n",
    "df_test = pd.read_csv('../data/processed/test.csv')\n",
    "q1_train_original = df_train['question1']\n",
    "q2_train_original = df_train['question2']\n",
    "q1_test_original = df_test['question1']\n",
    "q2_test_original = df_test['question2']\n",
    "\n",
    "del df_train\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Helper functions\n",
    "'''\n",
    "\n",
    "def get_function_name(f):\n",
    "    return str(f).split(' ')[1]\n",
    "\n",
    "def get_regex_number(regex, text):\n",
    "    m = re.findall(regex, text)\n",
    "    if m==None:\n",
    "        return 0\n",
    "    return len(m)\n",
    "\n",
    "'''\n",
    "Features\n",
    "'''\n",
    "\n",
    "def get_non_ascii(text):\n",
    "    return get_regex_number('[^\\x00-\\x7F]', text)\n",
    "    \n",
    "def get_number(text):\n",
    "    return get_regex_number('[0-9]+[\\.\\,]*[0-9]*', text)\n",
    "    \n",
    "def get_puncts(text):\n",
    "    return get_regex_number('[\\!\\?！？\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"’`“…é\\$\\%\\&]', text)\n",
    "\n",
    "def get_brackets(text):\n",
    "    return get_regex_number('[\\(\\)\\[\\]\\{\\}\\<\\>\\'\\\"]', text)\n",
    "\n",
    "def get_dashes(text):\n",
    "    return get_regex_number('\\-', text)\n",
    "\n",
    "def get_dots(text):\n",
    "    return get_regex_number('\\.', text)\n",
    "\n",
    "def get_end_of_sent(text):\n",
    "    return get_regex_number('[\\.\\!\\?！？]', text)\n",
    "\n",
    "def get_commas(text):\n",
    "    return get_regex_number('\\,', text)\n",
    "\n",
    "def get_spaces(text):\n",
    "    return get_regex_number('[\\s\\t\\n]+', text)\n",
    "\n",
    "def get_entity_count(text):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.ent_type_!='':\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def get_OOB(text):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if not token.has_vector:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "features_before_clean = [\n",
    "    get_non_ascii,\n",
    "    get_number,\n",
    "    get_puncts,\n",
    "    get_dashes,\n",
    "    get_dots,\n",
    "    get_end_of_sent,\n",
    "    get_commas,\n",
    "    get_spaces,\n",
    "    get_entity_count,\n",
    "    get_OOB,\n",
    "    get_brackets,\n",
    "]\n",
    "\n",
    "features_after_clean = [\n",
    "    get_non_ascii,\n",
    "    get_number,\n",
    "    get_puncts,\n",
    "    get_dashes,\n",
    "    get_dots,\n",
    "    get_end_of_sent,\n",
    "    get_commas,\n",
    "    get_entity_count,\n",
    "    get_OOB,\n",
    "    get_brackets,\n",
    "]\n",
    "\n",
    "def extract_features(texts, functions, run_name, q):\n",
    "    \n",
    "    fields = [run_name+'_'+q+'_'+get_function_name(f) for f in functions]\n",
    "    \n",
    "    sample_len = len(texts)\n",
    "    print('At '+run_name, sample_len, ' samples')\n",
    "    \n",
    "    ret = []\n",
    "    for i,text in enumerate(texts):\n",
    "        if type(text)!=str:\n",
    "            text = ''\n",
    "        features = [func(text) for func in functions]\n",
    "        ret.append(features)\n",
    "        \n",
    "        if i%100000==0:\n",
    "            print(i,'/',sample_len)\n",
    "        \n",
    "    return pd.DataFrame(data=ret, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q1_train_corrected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6681ade52578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'word_corrected'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq1_train_corrected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_after_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'q1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mq2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq2_train_corrected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_after_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'q2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'q1_train_corrected' is not defined"
     ]
    }
   ],
   "source": [
    "run_name = 'word_corrected'\n",
    "\n",
    "q1 = extract_features(q1_train_corrected, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_corrected, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_corrected, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_corrected, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "#######################\n",
    "\n",
    "run_name = 'simple_tokenizer'\n",
    "\n",
    "q1 = extract_features(q1_train_simple, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_simple, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_simple, features_after_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_simple, features_after_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "#######################\n",
    "\n",
    "run_name = 'raw'\n",
    "\n",
    "q1 = extract_features(q1_train_original, features_before_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_train_original, features_before_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)\n",
    "\n",
    "q1 = extract_features(q1_test_original, features_before_clean, run_name, q='q1')\n",
    "q2 = extract_features(q2_test_original, features_before_clean, run_name, q='q2')\n",
    "c = pd.concat([q1,q2], axis=1)\n",
    "c.to_csv('../data/interim/Key_features_'+run_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
